{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA NTL 1: Data Collecting & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ee\n",
    "import folium\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Population Density Variable 2015 - 2020 (Not used)\n",
    "##### No monthly data on this, so may not end up using in final regressions if I cant aggregate it at the same level as the other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from U.S. Census QuickFacts (all data sources, help resources used, & people worked with are linked at bottom of notebook)\n",
    "# No files were available to import, just assigned each year a value that the census listed as the population for that year\n",
    "popdensity = []\n",
    "\n",
    "pop15 = 8550405\n",
    "pop16 = 8537673\n",
    "pop17 = 8622698\n",
    "pop18 = 8398750\n",
    "pop19 = 8336817\n",
    "pop20 = 8804190\n",
    "\n",
    "pops = [8550405,8537673,8622698,8398750,8336817,8804190]\n",
    "\n",
    "# Calculating pop density by year\n",
    "for i in pops:\n",
    "    density = i/300.45\n",
    "    popdensity.append(density)\n",
    "popdensity\n",
    "\n",
    "# Turning into df\n",
    "# Corresponding years starting from 2015\n",
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "# Creating the df with column names\n",
    "population_density = pd.DataFrame({'Year': years, 'Population Density': popdensity})\n",
    "\n",
    "print(population_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly Metro Card Counts 2015 - 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Data.ny.gov MTA NYCT MetroCard History: 2010 - 2021 Dataset\n",
    "\n",
    "metroride = pd.read_csv(\"/Users/isabellagermani/Desktop/Projects/NASA Nighttime Lights & MTA Infrastructure/NYC MetroCard History 2010-2021.csv\")\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "columns_to_drop = [\"From Date\", \"Remote Station ID\", \"Station\", \"Student\", \"NICE 2-Trip\", \"CUNY 120 Day\", \"CUNY 60 Day\", \"Fair Fares Pay-Per-Ride\", \"Fair Fares 7 Day Unlimited\", \"Fair Fares 30 Day Unlimited\"]\n",
    "metroride.drop(columns=columns_to_drop, inplace=True)\n",
    "metroride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding counts of all Metro Card Types together to see overall effect\n",
    "metroridetotals = []\n",
    "\n",
    "# For loop iterating through metroride rows \n",
    "for index, row in metroride.iterrows():\n",
    "    metroridet = row['Full Fare'] + row['Senior Citizen/Disabled'] + row['7 Day ADA Farecard Access System Unlimited'] + row['30 Day ADA Farecard Access/Reduced Fare Media Unlimited'] + row['Joint Rail Road Ticket'] + row['7 Day Unlimited'] + row['30 Day Unlimited'] + row['14 Day Reduced Fare Media Unlimited'] + row['1 Day Unlimited'] + row['14 Day Unlimited'] + row['7 Day Express Bus Pass'] + row['TransitCheck'] + row['Long Island Bus Special Senior'] + row['Reduced Fare 2-Trip'] + row['Rail Road Unlimited No Trade'] + row['TransitCheck Annual'] + row['Mail and Ride EasyPay Express'] + row['Mail and Ride EasyPay Unlimited'] + row['PATH 2-Trip'] + row['AirTrain Full Fare'] + row['AirTrain 30 Day Unlimited'] + row['AirTrain 10-Trip'] + row['AirTrain Monthly']\n",
    "    metroridetotals.append(metroridet)\n",
    "\n",
    "# Creating a new df with this info\n",
    "new_metro = pd.DataFrame({'Date': metroride['To Date'], 'Total Counts': metroridetotals})\n",
    "\n",
    "# Converting Date column to datetime format\n",
    "new_metro['Date'] = pd.to_datetime(new_metro['Date'])\n",
    "\n",
    "# Sorting by Date column in ascending order\n",
    "new_metro = new_metro.sort_values(by='Date')\n",
    "\n",
    "# Filtering rows within the year range 2015 - 2020\n",
    "new_metro = new_metro[(new_metro['Date'].dt.year >= 2015) & (new_metro['Date'].dt.year <= 2020)]\n",
    "\n",
    "# Resetting index after dropping rows\n",
    "new_metro = new_metro.reset_index(drop=True)\n",
    "new_metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: Making sure I cant fuck with the original df\n",
    "NEW_METRO = new_metro.copy()\n",
    "\n",
    "# Defining new month and year columns for next cell's code\n",
    "NEW_METRO['Month'] = NEW_METRO['Date'].dt.strftime('%m')\n",
    "NEW_METRO['Year'] = NEW_METRO['Date'].dt.strftime('%Y')\n",
    "\n",
    "# Concatenating Year & Month columns to make YearMonth column (to keep for end dataset: same format as NASA data, easier to aggregate)\n",
    "NEW_METRO['YearMonth'] = NEW_METRO['Year'] + '-' + NEW_METRO['Month']\n",
    "NEW_METRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'YearMonth' and calculating the average of 'Total Counts' for each month/year combo\n",
    "MTA = NEW_METRO.groupby('YearMonth')['Total Counts'].mean().reset_index()\n",
    "\n",
    "# Renaming columns\n",
    "MTA.rename(columns={'Total Counts': 'Avg Monthly Metro Card Count'}, inplace=True)\n",
    "\n",
    "MTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTA Major Incidents 2015 - 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from data.ny.gov Metropolitan Transportation Authority Performance Indicators per Agency: 2008-2021 report\n",
    "\n",
    "incidents1519 = pd.read_csv('/Users/isabellagermani/Desktop/Projects/NASA Nighttime Lights & MTA Infrastructure/MTA Subway Major Incidents 2015-2019.csv')\n",
    "incidents20 = pd.read_csv('/Users/isabellagermani/Desktop/Projects/NASA Nighttime Lights & MTA Infrastructure/MTA Subway Major Incidents 2020.csv')\n",
    "\n",
    "# Concatenating both datasets into one\n",
    "incidents = pd.concat([incidents1519,incidents20], ignore_index=True)\n",
    "incidents\n",
    "\n",
    "# Dropping rows outside date range (defining date range)\n",
    "start_year = '2015'\n",
    "end_year = '2020'\n",
    "\n",
    "# Filtering to keep rows within desired year range\n",
    "filtered_incidents = incidents[incidents['month'].str[:4].between(start_year, end_year)]\n",
    "\n",
    "filtered_incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by month\n",
    "filtered_incidents_sorted = filtered_incidents.sort_values(by='month')\n",
    "filtered_incidents_sorted\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "columns_to_drop = [\"division\",\"line\",\"day_type\"]\n",
    "filtered_incidents_sorted.drop(columns=columns_to_drop, inplace=True)\n",
    "filtered_incidents_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: Working with copy df now\n",
    "ICS = filtered_incidents_sorted.copy()\n",
    "\n",
    "# Assigning numerical ID to each category name\n",
    "category_ids, category_labels = pd.factorize(ICS['category']) # Getting unique strings and assigning their corresponding numerical IDs\n",
    "\n",
    "# Changing column name\n",
    "ICS.rename(columns={'month': 'YearMonth'}, inplace=True)\n",
    "\n",
    "# Adding categoryID column to df\n",
    "ICS['categoryID'] = category_ids\n",
    "ICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by YearMonth and categoryID, and calculating the count frequencies all in 1\n",
    "ICS['count_frequency'] = ICS.groupby(['YearMonth', 'categoryID'])['count'].transform('sum')\n",
    "ICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting YearMonth to datetime\n",
    "ICS['YearMonth'] = pd.to_datetime(ICS['YearMonth'])\n",
    "\n",
    "# Extracting year and month to separate columns\n",
    "ICS['Year'] = ICS['YearMonth'].dt.year\n",
    "ICS['Month'] = ICS['YearMonth'].dt.month\n",
    "\n",
    "# Creating a pivot table with count frequencies for each category\n",
    "pivot_table = ICS.pivot_table(index=['Year', 'Month'], columns='category', values='count_frequency', aggfunc='sum') #*\n",
    "\n",
    "# Reseting index to make Year and Month regular columns\n",
    "pivot_table.reset_index(inplace=True)\n",
    "\n",
    "# Filling NaN values with 0\n",
    "pivot_table.fillna(0, inplace=True)\n",
    "\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCIDENTS = pd.DataFrame(pivot_table)\n",
    "\n",
    "# Regenerating YearMonth column (dk where it went & why its not here)\n",
    "# Converting Month to string format\n",
    "INCIDENTS['Month'] = INCIDENTS['Month'].astype(str)\n",
    "\n",
    "# Combining Year and Month columns with leading zeros to create 'YearMonth' for aggregation\n",
    "INCIDENTS['YearMonth'] = INCIDENTS['Year'].astype(str) + '-' + INCIDENTS['Month'].str.zfill(2)\n",
    "\n",
    "# Dropping Unnecessary columns\n",
    "INCIDENTS.drop(columns='Year', inplace=True)\n",
    "INCIDENTS.drop(columns='Month', inplace=True)\n",
    "\n",
    "# Adding one more column to represent total number of major incidents\n",
    "INCIDENTS['Total_n_Incidents'] = INCIDENTS.drop(columns='YearMonth').sum(axis=1)\n",
    "INCIDENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last minute column name changes\n",
    "INCIDENTS.rename(columns={'Other': 'NIncidents: Other'}, inplace=True)\n",
    "INCIDENTS.rename(columns={'Persons on Trackbed/Police/Medical': 'NIncidents: Police/Medical/Person on Trackbed'}, inplace=True)\n",
    "INCIDENTS.rename(columns={'Signals': 'NIncidents: Signals'}, inplace=True)\n",
    "INCIDENTS.rename(columns={'Stations and Structure': 'NIncidents: Stations and Structure'}, inplace=True)\n",
    "INCIDENTS.rename(columns={'Subway Car': 'NIncidents: Subway Car'}, inplace=True)\n",
    "INCIDENTS.rename(columns={'Track': 'NIncidents: Track'}, inplace=True)\n",
    "INCIDENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NASA Nighttime Lights Data 2015 - 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data obtained from NASA's VNP46a2 product data (Gap_Filled_DNB_BRDF_Corrected_NTL band) via Google Earth Engine API\n",
    "\n",
    "# Authentication\n",
    "ee.Authenticate()\n",
    "\n",
    "# Initializing cloud project\n",
    "ee.Initialize(project='nasantl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & ROI Visualization\n",
    "\n",
    "# Defining NYC as region of interest using a geometry polygon and estimated coordinates\n",
    "roi = ee.Geometry.Polygon([\n",
    "  [-74.03, 40.91],  # Staten Island (upper left)\n",
    "  [-73.90, 40.91],  # Upper Manhattan\n",
    "  [-73.90, 40.69],  # Lower Manhattan\n",
    "  [-74.03, 40.50],  # Brooklyn\n",
    "  [-73.78, 40.50],  # Queens\n",
    "  [-73.78, 40.91],  # Bronx (upper right)\n",
    "  [-74.03, 40.91]   # Close the polygon\n",
    "])\n",
    "\n",
    "# Wasnt able to properly group data by borough, but would definitely include in a revision \n",
    "# # Define polygons for NYC boroughs\n",
    "# si_roi = ee.Geometry.Polygon([\n",
    "#     [-74.259, 40.648],  # South West\n",
    "#     [-74.043, 40.648],  # South East\n",
    "#     [-74.043, 40.517],  # North East\n",
    "#     [-74.259, 40.517],  # North West\n",
    "#     [-74.259, 40.648]   # Close the polygon\n",
    "# ])\n",
    "\n",
    "# m_roi = ee.Geometry.Polygon([\n",
    "#     [-74.026, 40.879],  # South West\n",
    "#     [-73.909, 40.879],  # South East\n",
    "#     [-73.909, 40.699],  # North East\n",
    "#     [-74.026, 40.699],  # North West\n",
    "#     [-74.026, 40.879]   # Close the polygon\n",
    "# ])\n",
    "\n",
    "# bk_roi = ee.Geometry.Polygon([\n",
    "#     [-74.043, 40.698],  # South West\n",
    "#     [-73.833, 40.698],  # South East\n",
    "#     [-73.833, 40.570],  # North East\n",
    "#     [-74.043, 40.570],  # North West\n",
    "#     [-74.043, 40.698]   # Close the polygon\n",
    "# ])\n",
    "\n",
    "# q_roi = ee.Geometry.Polygon([\n",
    "#     [-73.962, 40.700],  # South West\n",
    "#     [-73.700, 40.700],  # South East\n",
    "#     [-73.700, 40.541],  # North East\n",
    "#     [-73.962, 40.541],  # North West\n",
    "#     [-73.962, 40.700]   # Close the polygon\n",
    "# ])\n",
    "\n",
    "# bx_roi = ee.Geometry.Polygon([\n",
    "#     [-73.933, 40.927],  # South West\n",
    "#     [-73.700, 40.927],  # South East\n",
    "#     [-73.700, 40.791],  # North East\n",
    "#     [-73.933, 40.791],  # North West\n",
    "#     [-73.933, 40.927]   # Close the polygon\n",
    "# ])\n",
    "\n",
    "# Defining variable containing filtered data\n",
    "data = ee.ImageCollection('NOAA/VIIRS/001/VNP46A2').filterDate('2015-01-01', '2020-12-31')\n",
    "\n",
    "# Selecting first image in dataset\n",
    "brdf = data.first()\n",
    "\n",
    "# Selecting correct Band for pixel values\n",
    "brdf = brdf.select('Gap_Filled_DNB_BRDF_Corrected_NTL')\n",
    "band_name = 'Gap_Filled_DNB_BRDF_Corrected_NTL'\n",
    "\n",
    "# Defining Visualization parameters\n",
    "brdfVis = {\n",
    "  'min': 0,\n",
    "  'max': 100,\n",
    "  'palette': ['black', 'purple', 'cyan', 'green', 'yellow', 'red', 'white'],\n",
    "  'region': roi\n",
    "}\n",
    "\n",
    "# Creating folium Map as object my_map using roi coordinates\n",
    "my_map = folium.Map(location=[40.71, -73.93], zoom_start=10)\n",
    "\n",
    "# Adding Google Earth Engine layer to map\n",
    "folium.TileLayer(\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='Gap_Filled_DNB_BRDF_Corrected_NTL'\n",
    ").add_to(my_map)\n",
    "\n",
    "# Displaying map\n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding number of images in the dataset\n",
    "num_images = data.size().getInfo() # have to include .getInfo() to call actual data for image\n",
    "print(\"Number of images:\", num_images)\n",
    "\n",
    "# Defining first image as the first object in the dataset\n",
    "first_image = ee.Image(data.first())\n",
    "\n",
    "# Print information about the first image\n",
    "print(\"Information about the first image:\")\n",
    "print(first_image.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into years\n",
    "data2015 = ee.ImageCollection('NOAA/VIIRS/001/VNP46A2').filterDate('2015-01-01', '2015-12-31')\n",
    "data2016 = ee.ImageCollection('NOAA/VIIRS/001/VNP46A2').filterDate('2016-01-01', '2016-12-31')\n",
    "data2017 = ee.ImageCollection('NOAA/VIIRS/001/VNP46A2').filterDate('2017-01-01', '2017-12-31')\n",
    "data2018 = ee.ImageCollection('NOAA/VIIRS/001/VNP46A2').filterDate('2018-01-01', '2018-12-31')\n",
    "data2019 = ee.ImageCollection('NOAA/VIIRS/001/VNP46A2').filterDate('2019-01-01', '2019-12-31')\n",
    "data2020 = ee.ImageCollection('NOAA/VIIRS/001/VNP46A2').filterDate('2020-01-01', '2020-12-31')\n",
    "\n",
    "# Defining eachs num_images\n",
    "num_images_15 = data2018.size().getInfo()\n",
    "num_images_16 = data2018.size().getInfo()\n",
    "num_images_17 = data2018.size().getInfo()\n",
    "num_images_18 = data2018.size().getInfo()\n",
    "num_images_19 = data2019.size().getInfo()\n",
    "num_images_20 = data2020.size().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing lists\n",
    "dates_2015 = [] # will contain the date (YYYY-MM-DD) image containing the pixel radiance value was taken\n",
    "pixels_2015 = [] # will contain the light radiance pixel value measuring the intensity of artificial light source on that date \n",
    "\n",
    "dates_2016 = []\n",
    "pixels_2016 = []\n",
    "\n",
    "dates_2017 = []\n",
    "pixels_2017 = []\n",
    "\n",
    "dates_2018 = []\n",
    "pixels_2018 = []\n",
    "\n",
    "dates_2019 = []\n",
    "pixels_2019 = []\n",
    "\n",
    "dates_2020 = []\n",
    "pixels_2020 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE YEAR 2015\n",
    "# Iterating over each image in the collection\n",
    "for i in range(num_images_15):\n",
    "\n",
    "    # Getting the current image info\n",
    "    image_15 = ee.Image(data2015.toList(num_images_15).get(i))\n",
    "    \n",
    "    # Getting system start time variable (formatted in milliseconds since unix epoch (January 1, 1970))\n",
    "    start_time_ms = image_15.get('system:time_start').getInfo()\n",
    "    \n",
    "    # Converting milliseconds to datetime format\n",
    "    start_time = datetime.datetime.utcfromtimestamp(start_time_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # Adding each date to list\n",
    "    dates_2015.append(start_time[:10])  # only gets first 9 characters (system:time_start var contains more info than YYYY-MM-DD)\n",
    "\n",
    "    # Specifying which band to collect pixel intensity from\n",
    "    band_name = 'Gap_Filled_DNB_BRDF_Corrected_NTL'\n",
    "    # Getting pixel intensity values\n",
    "    intensity = image_15.select(band_name).reduceRegion(reducer=ee.Reducer.mean(), geometry=roi, scale=30).get(band_name).getInfo()   \n",
    "    # Adding each pixel value to list\n",
    "    pixels_2015.append(intensity)\n",
    "\n",
    "# Printing Lists\n",
    "print(pixels_2015)\n",
    "print(dates_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE YEAR 2016\n",
    "# Iterating over each image in the collection\n",
    "for i in range(num_images_16):\n",
    "\n",
    "    # Getting the current image info\n",
    "    image_16 = ee.Image(data2016.toList(num_images_16).get(i))\n",
    "    \n",
    "    # Getting system start time variable (formatted in milliseconds since unix epoch (January 1, 1970))\n",
    "    start_time_ms = image_16.get('system:time_start').getInfo()\n",
    "    \n",
    "    # Converting milliseconds to datetime format\n",
    "    start_time = datetime.datetime.utcfromtimestamp(start_time_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # Adding each date to list\n",
    "    dates_2016.append(start_time[:10]) \n",
    "\n",
    "    # Specifying which band to collect pixel intensity from\n",
    "    band_name = 'Gap_Filled_DNB_BRDF_Corrected_NTL'\n",
    "    # Getting pixel intensity values\n",
    "    intensity = image_16.select(band_name).reduceRegion(reducer=ee.Reducer.mean(), geometry=roi, scale=30).get(band_name).getInfo()   \n",
    "    # Adding each pixel value to list\n",
    "    pixels_2016.append(intensity)\n",
    "\n",
    "# Printing Lists\n",
    "print(pixels_2016)\n",
    "print(dates_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE YEAR 2017\n",
    "# Iterating over each image in the collection\n",
    "for i in range(num_images_17):\n",
    "\n",
    "    # Getting the current image info\n",
    "    image_17 = ee.Image(data2017.toList(num_images_17).get(i))\n",
    "    \n",
    "    # Getting system start time variable (formatted in milliseconds since unix epoch (January 1, 1970))\n",
    "    start_time_ms = image_17.get('system:time_start').getInfo()\n",
    "    \n",
    "    # Converting milliseconds to datetime format\n",
    "    start_time = datetime.datetime.utcfromtimestamp(start_time_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # Adding each date to list\n",
    "    dates_2017.append(start_time[:10]) \n",
    "\n",
    "    # Specifying which band to collect pixel intensity from\n",
    "    band_name = 'Gap_Filled_DNB_BRDF_Corrected_NTL'\n",
    "    # Getting pixel intensity values\n",
    "    intensity = image_17.select(band_name).reduceRegion(reducer=ee.Reducer.mean(), geometry=roi, scale=30).get(band_name).getInfo()   \n",
    "    # Adding each pixel value to list\n",
    "    pixels_2017.append(intensity)\n",
    "\n",
    "# Printing Lists\n",
    "print(pixels_2017)\n",
    "print(dates_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE YEAR 2018\n",
    "# Iterating over each image in the collection\n",
    "for i in range(num_images_18):\n",
    "\n",
    "    # Getting the current image info\n",
    "    image_18 = ee.Image(data2018.toList(num_images_18).get(i))\n",
    "    \n",
    "    # Getting system start time variable (formatted in milliseconds since unix epoch (January 1, 1970))\n",
    "    start_time_ms = image_18.get('system:time_start').getInfo()\n",
    "    \n",
    "    # Converting milliseconds to datetime format\n",
    "    start_time = datetime.datetime.utcfromtimestamp(start_time_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # Adding each date to list\n",
    "    dates_2018.append(start_time[:10]) \n",
    "\n",
    "    # Specifying which band to collect pixel intensity from\n",
    "    band_name = 'Gap_Filled_DNB_BRDF_Corrected_NTL'\n",
    "    # Getting pixel intensity values\n",
    "    intensity = image_18.select(band_name).reduceRegion(reducer=ee.Reducer.mean(), geometry=roi, scale=30).get(band_name).getInfo()   \n",
    "    # Adding each pixel value to list\n",
    "    pixels_2018.append(intensity)\n",
    "\n",
    "# Printing Lists\n",
    "print(pixels_2018)\n",
    "print(dates_2018) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE YEAR 2019\n",
    "# Iterating over each image in the collection\n",
    "for i in range(num_images_19):\n",
    "\n",
    "    # Getting the current image info\n",
    "    image_19 = ee.Image(data2019.toList(num_images_19).get(i))\n",
    "    \n",
    "    # Getting system start time variable (formatted in milliseconds since unix epoch (January 1, 1970))\n",
    "    start_time_ms = image_19.get('system:time_start').getInfo()\n",
    "    \n",
    "    # Converting milliseconds to datetime format\n",
    "    start_time = datetime.datetime.utcfromtimestamp(start_time_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # Adding each date to list\n",
    "    dates_2019.append(start_time[:10])\n",
    "    # Specifying which band to collect pixel intensity from\n",
    "    band_name = 'Gap_Filled_DNB_BRDF_Corrected_NTL'\n",
    "    # Getting pixel intensity values\n",
    "    intensity = image_19.select(band_name).reduceRegion(reducer=ee.Reducer.mean(), geometry=roi, scale=30).get(band_name).getInfo()   \n",
    "    # Adding each pixel value to list\n",
    "    pixels_2019.append(intensity)\n",
    "\n",
    "# Printing Lists\n",
    "print(pixels_2019)\n",
    "print(dates_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE YEAR 2020\n",
    "\n",
    "# Iterating over each image in the collection\n",
    "for i in range(num_images_20):\n",
    "\n",
    "    # Getting the current image info\n",
    "    image_20 = ee.Image(data2020.toList(num_images_20).get(i))\n",
    "    \n",
    "    # Getting system start time variable\n",
    "    start_time_ms = image_20.get('system:time_start').getInfo()\n",
    "    \n",
    "    # Converting milliseconds to datetime format\n",
    "    start_time = datetime.datetime.utcfromtimestamp(start_time_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # Adding each date to list\n",
    "    dates_2020.append(start_time[:10])\n",
    "\n",
    "    # Specifying which band to collect pixel intensity from\n",
    "    band_name = 'Gap_Filled_DNB_BRDF_Corrected_NTL'\n",
    "    # Getting pixel intensity values\n",
    "    intensity = image_20.select(band_name).reduceRegion(reducer=ee.Reducer.mean(), geometry=roi, scale=30).get(band_name).getInfo()   \n",
    "    # Adding each pixel value to list\n",
    "    pixels_2020.append(intensity)\n",
    "\n",
    "# Printing Lists\n",
    "print(pixels_2020)\n",
    "print(dates_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasting lists together to create pandas df\n",
    "\n",
    "# Concatenating all of the lists\n",
    "All_Pixels = pixels_2015 + pixels_2016 + pixels_2017 + pixels_2018 + pixels_2019 + pixels_2020 \n",
    "All_Dates = dates_2015 + dates_2016 + dates_2017 + dates_2018 + dates_2019 + dates_2020 \n",
    "\n",
    "# # Dropping 2 values from dates (causes formatting issue otherwise)\n",
    "All_Dates = All_Dates[:-2]\n",
    "\n",
    "# Creating df that has 2 columns \"Date\" & \"Light_intensity\" and uses the list info for column values\n",
    "NASA = pd.DataFrame({'Date': All_Dates, 'Light_intensity': All_Pixels})\n",
    "NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: Making sure I cant use the original df\n",
    "NTL1 = NASA.copy()\n",
    "\n",
    "# Making avg light intensity per month variable & adjusting date column to only show year & month\n",
    "# Getting year and month from Date column\n",
    "NTL1['YearMonth'] = NTL1['Date'].str[:7]\n",
    "\n",
    "# Converting light intensity to numeric value\n",
    "NTL1['Light_intensity'] = pd.to_numeric(NTL1['Light_intensity'], errors='coerce') #*\n",
    "\n",
    "# Grouping by YearMonth and calculating avg light intensity\n",
    "NTL = NTL1.groupby('YearMonth')['Light_intensity'].mean().reset_index()\n",
    "NTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: Making copies again for backup\n",
    "NASA_NTL = NTL.copy()\n",
    "Metro_Cards = MTA.copy()\n",
    "Major_Incidents = INCIDENTS.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging on YearMonth column\n",
    "MERGED = NASA_NTL.merge(Metro_Cards, on='YearMonth', how='outer').merge(Major_Incidents, on='YearMonth', how='outer')\n",
    "MERGED\n",
    "\n",
    "# Replacing NaN values w/ 0s\n",
    "MERGED.fillna(0, inplace=True)\n",
    "\n",
    "# Removing trailing zeros from float columns\n",
    "MERGED = MERGED.apply(lambda x: x.astype(int) if x.name != 'YearMonth' else x)\n",
    "\n",
    "# Renaming again\n",
    "MERGED.rename(columns={'YearMonth': 'Month'}, inplace=True)\n",
    "MERGED.rename(columns={'NIncidents: Other': 'Number of incidents: Other'}, inplace=True)\n",
    "MERGED.rename(columns={'NIncidents: Police/Medical/Person on Trackbed': 'Number of incidents: Police/Medical/Person on Track'}, inplace=True)\n",
    "MERGED.rename(columns={'NIncidents: Signals': 'Number of incidents: Signals'}, inplace=True)\n",
    "MERGED.rename(columns={'NIncidents: Stations and Structure': 'Number of incidents: Stations and Structure'}, inplace=True)\n",
    "MERGED.rename(columns={'NIncidents: Subway Car': 'Number of incidents: Subway Car'}, inplace=True)\n",
    "MERGED.rename(columns={'NIncidents: Track': 'Number of incidents: Track'}, inplace=True)\n",
    "MERGED.rename(columns={'Total_n_Incidents': 'Total Number of incidents'}, inplace=True)\n",
    "MERGED.rename(columns={'Light_intensity': 'Avg Light Intensity Value'}, inplace=True)\n",
    "MERGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORTING\n",
    "\n",
    "# Defining location path to save CSV file to\n",
    "desired_dir = '/Users/isabellagermani/Desktop/Projects/NASA Nighttime Lights & MTA Infrastructure'\n",
    "\n",
    "# Checking current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current working directory:\", current_dir)\n",
    "\n",
    "# Changing the working directory if it's different from the desired directory\n",
    "if current_dir != desired_dir:\n",
    "    os.chdir(desired_dir)\n",
    "    print(\"Changed working directory to:\", desired_dir)\n",
    "\n",
    "# Exporting dataframe to CSV \n",
    "MERGED.to_csv('merged_dataset.csv', index=False)\n",
    "MERGED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One regression I plan to run is metro card count on light intensity. I hypothesize that as the light intensity increases, number of MTA riders will also rise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Help\n",
    "grouping by both month and year: https://stackoverflow.com/questions/26646191/pandas-groupby-month-and-year \n",
    "\n",
    ".loc and .iloc: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html \n",
    "\n",
    "Resetting indexes: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html\n",
    "\n",
    "Iterating over pandas df rows: https://pythonexamples.org/pandas-dataframe-iterate-rows-iterrows/ \n",
    "\n",
    "folium documentation: https://python-visualization.github.io/folium/latest/user_guide.html\n",
    "\n",
    "bbox coordinates: https://gis.stackexchange.com/questions/255158/get-minimum-and-maximum-latitude-and-longitude-of-new-york\n",
    "\n",
    "geodataframe resource: https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html\n",
    "\n",
    "homebrew: https://www.digitalocean.com/community/tutorials/how-to-install-and-use-homebrew-on-macos\n",
    "\n",
    "geometry rectangle: https://developers.google.com/earth-engine/apidocs/ee-geometry-rectangle \n",
    "\n",
    "### # Metro Card Counts Variable\n",
    "Data source: https://data.ny.gov/Transportation/MTA-NYCT-MetroCard-History-2010-2021/v7qc-gwpn/about_data \n",
    "\n",
    "### MTA Major Subway Incidents Variable\n",
    "15-19 Data Source: https://data.ny.gov/Transportation/MTA-Subway-Major-Incidents-2015-2019/ereg-mcvp/about_data\n",
    "2020 Data Source: https://data.ny.gov/Transportation/MTA-Subway-Major-Incidents-Beginning-2020/j6d2-s8m2/about_data\n",
    "\n",
    "### Social Vulnerability Data\n",
    "Components of Social Vulnerability: https://experience.arcgis.com/experience/b0341fa9b237456c9a9f1758c15cde8d/ \n",
    "\n",
    "Variables in Census dataset: https://api.census.gov/data/2022/cre/variables.html\n",
    "\n",
    "Example API Census calls: https://api.census.gov/data/2022/cre/examples.html \n",
    "\n",
    "### NASA NTL Data / Google Earth Engine (method of NTL data download)\n",
    "NASA BlackMarble user guide: https://viirsland.gsfc.nasa.gov/PDF/BlackMarbleUserGuide_v1.2_20220916.pdf\n",
    "\n",
    "NASA useful tools: https://blackmarble.gsfc.nasa.gov/Tools.html\n",
    "\n",
    "NTL dataset Introduction: https://appliedsciences.nasa.gov/sites/default/files/2020-12/BlackMarble_2020.pdf\n",
    "\n",
    "Python script download resource: https://ladsweb.modaps.eosdis.nasa.gov/tools-and-services/data-download-scripts/#wget\n",
    "\n",
    "EarthData Forum: https://forum.earthdata.nasa.gov/viewtopic.php?t=4532 \n",
    "\n",
    "How to use Google Earth Engine API for NTL data: https://gis.stackexchange.com/questions/460447/download-nasas-black-marble-vnpa2-nighttime-light-data-using-google-earth-engin \n",
    "\n",
    "Earth Engine NTL documentation: https://developers.google.com/earth-engine/datasets/catalog/NOAA_VIIRS_001_VNP46A2#code-editor-javascript \n",
    "\n",
    "Image Visualization & Bands: https://developers.google.com/earth-engine/tutorials/tutorial_api_02 \n",
    "\n",
    "Google Earth Engine Cloud Product Portal: https://console.cloud.google.com/apis/api/earthengine.googleapis.com/credentials?project=nasantl \n",
    "\n",
    "How to compute from image data: https://developers.google.com/earth-engine/tutorials/tutorial_api_03\n",
    "\n",
    "Figuring out what product output looks like & how to use it for this research question: https://www.youtube.com/watch?v=KSxlhBLOAc4\n",
    "\n",
    "### Population Density Data\n",
    "US Census QuickFacts: https://www.census.gov/quickfacts/fact/table/newyorkcitynewyork/PST045223#PST045223\n",
    "\n",
    "U.S. Census Population Estimates: https://www.nyc.gov/assets/planning/download/pdf/planning-level/nyc-population/population-estimates/current-population-estimates-2023.pdf\n",
    "\n",
    "NYS department of health population estimates: https://www.health.ny.gov/statistics/vital_statistics/2017/table02.html\n",
    "\n",
    "Total Land Area Value: https://www.census.gov/quickfacts/fact/table/newyorkcitynewyork/LND110220\n",
    "\n",
    "### people worked with: Teja Vuppu, multiple learning commons students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
